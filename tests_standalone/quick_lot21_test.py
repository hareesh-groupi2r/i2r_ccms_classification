#!/usr/bin/env python3
"""
Quick LOT-21 Test - Test improvements on just 5 representative documents
"""

import sys
import logging
from pathlib import Path
import time
import json
from datetime import datetime

# Add the classification system to path
sys.path.insert(0, str(Path(__file__).parent))

from classifier.config_manager import ConfigManager
from classifier.issue_mapper import IssueCategoryMapper  
from classifier.validation import ValidationEngine
from classifier.data_sufficiency import DataSufficiencyAnalyzer
from classifier.hybrid_rag import HybridRAGClassifier

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

def quick_test_improvements():
    """Quick test of improvements on 5 representative LOT-21 documents"""
    
    logger.info("="*80)
    logger.info("QUICK TEST: IMPROVED HYBRID RAG vs ORIGINAL RESULTS")
    logger.info("="*80)
    
    # Representative documents from different types
    test_documents = [
        "data/Lots21-27/Lot 21 to 23/LOT-21/1. AE to SPK ltr no 1963 dt 16.09.2017.pdf",
        "data/Lots21-27/Lot 21 to 23/LOT-21/10. 2917 - Final Payment Statement Submission (EPC10).pdf", 
        "data/Lots21-27/Lot 21 to 23/LOT-21/5. MOM - Project Review Meeting dt 22.05.2018.pdf",
        "data/Lots21-27/Lot 21 to 23/LOT-21/17. PD_Applicability of GST in Construction Service in Highways and Road Works.pdf",
        "data/Lots21-27/Lot 21 to 23/LOT-21/13. Price Adjustment.PDF"
    ]
    
    # Initialize the improved classifier
    logger.info("Initializing improved classification system...")
    
    config_manager = ConfigManager()
    hybrid_config = config_manager.get_approach_config('hybrid_rag')
    
    # Show updated configuration
    logger.info("IMPROVED CONFIGURATION:")
    logger.info(f"  similarity_threshold: {hybrid_config.get('similarity_threshold', 0.65)} (was 0.3)")
    logger.info(f"  top_k: {hybrid_config.get('top_k', 8)} (was 15)")
    logger.info(f"  max_issues: {hybrid_config.get('max_issues', 5)} (was 10)")
    logger.info(f"  document_context_chars: {hybrid_config.get('document_context_chars', 2000)} (was 1500)")
    
    # Find training data
    training_data_path = 'data/synthetic/combined_training_data.xlsx'
    if not Path(training_data_path).exists():
        training_data_path = 'data/raw/Consolidated_labeled_data.xlsx'
    
    # Initialize components
    issue_mapper = IssueCategoryMapper(training_data_path)
    validator = ValidationEngine(training_data_path)
    data_analyzer = DataSufficiencyAnalyzer(training_data_path)
    
    classifier = HybridRAGClassifier(
        config=hybrid_config,
        issue_mapper=issue_mapper,
        validator=validator,
        data_analyzer=data_analyzer
    )
    
    # Build or load index
    index_path = Path('data/embeddings/rag_index')
    if not index_path.with_suffix('.faiss').exists():
        logger.info("Building vector index...")
        classifier.build_index(training_data_path, save_path=str(index_path))
    
    results = []
    total_time = 0
    
    # Process each document
    for i, doc_path in enumerate(test_documents, 1):
        if not Path(doc_path).exists():
            logger.warning(f"Document not found: {doc_path}")
            continue
            
        logger.info(f"\n{'='*60}")
        logger.info(f"TESTING DOCUMENT {i}: {Path(doc_path).name}")
        logger.info(f"{'='*60}")
        
        start_time = time.time()
        
        try:
            result = classifier.classify(doc_path, is_file_path=True)
            processing_time = time.time() - start_time
            total_time += processing_time
            
            # Extract key metrics
            issues = result.get('identified_issues', [])
            categories = result.get('categories', [])
            
            doc_result = {
                'document': Path(doc_path).name,
                'processing_time': processing_time,
                'success': True,
                'issues_count': len(issues),
                'categories_count': len(categories),
                'max_issue_confidence': max([i.get('confidence', 0) for i in issues], default=0),
                'max_category_confidence': max([c.get('confidence', 0) for c in categories], default=0),
                'top_issues': [{'type': i.get('issue_type', ''), 'confidence': i.get('confidence', 0)} 
                              for i in issues[:3]],
                'top_categories': [{'type': c.get('category', ''), 'confidence': c.get('confidence', 0)} 
                                  for c in categories[:3]]
            }
            
            logger.info(f"‚úÖ SUCCESS - {processing_time:.2f}s")
            logger.info(f"   Issues: {len(issues)} (max conf: {doc_result['max_issue_confidence']:.3f})")
            logger.info(f"   Categories: {len(categories)} (max conf: {doc_result['max_category_confidence']:.3f})")
            
            if issues:
                logger.info("   Top Issues:")
                for j, issue in enumerate(issues[:3], 1):
                    logger.info(f"     {j}. {issue.get('issue_type', 'Unknown')} ({issue.get('confidence', 0):.3f})")
                    
            results.append(doc_result)
            
        except Exception as e:
            processing_time = time.time() - start_time
            total_time += processing_time
            
            logger.error(f"‚ùå FAILED - {str(e)}")
            results.append({
                'document': Path(doc_path).name,
                'processing_time': processing_time,
                'success': False,
                'error': str(e)
            })
    
    # Generate comparison report
    logger.info("\n" + "üéØ" + "="*78 + "üéØ")
    logger.info("                       QUICK TEST RESULTS SUMMARY")
    logger.info("üéØ" + "="*78 + "üéØ")
    
    successful_results = [r for r in results if r.get('success', False)]
    
    if successful_results:
        # Metrics
        avg_processing_time = sum(r['processing_time'] for r in successful_results) / len(successful_results)
        avg_issues = sum(r['issues_count'] for r in successful_results) / len(successful_results)
        avg_categories = sum(r['categories_count'] for r in successful_results) / len(successful_results)
        avg_confidence = sum(max(r['max_issue_confidence'], r['max_category_confidence']) 
                           for r in successful_results) / len(successful_results)
        
        logger.info(f"üìä PERFORMANCE METRICS:")
        logger.info(f"   Successful Documents: {len(successful_results)}/{len(results)}")
        logger.info(f"   Average Processing Time: {avg_processing_time:.2f}s")
        logger.info(f"   Total Processing Time: {total_time:.2f}s")
        
        logger.info(f"\nüîç CLASSIFICATION METRICS:")
        logger.info(f"   Average Issues per Document: {avg_issues:.1f}")
        logger.info(f"   Average Categories per Document: {avg_categories:.1f}")
        logger.info(f"   Average Max Confidence: {avg_confidence:.3f}")
        
        # Compare with original results (from previous batch)
        logger.info(f"\nüìà EXPECTED IMPROVEMENTS:")
        logger.info(f"   üéØ Precision: Expected HIGHER due to similarity_threshold 0.3 ‚Üí 0.65")
        logger.info(f"   üîç False Positives: Expected REDUCED by ~60-70%")
        logger.info(f"   ‚ö° Processing Speed: Expected FASTER due to top_k 15 ‚Üí 8")
        logger.info(f"   üõ°Ô∏è  Quality: Enhanced with confidence decay and filtering")
        
        # Detailed results
        logger.info(f"\nüìã DOCUMENT-BY-DOCUMENT RESULTS:")
        for i, result in enumerate(successful_results, 1):
            logger.info(f"   {i}. {result['document']}")
            logger.info(f"      Time: {result['processing_time']:.2f}s, Issues: {result['issues_count']}, Categories: {result['categories_count']}")
            if result['top_issues']:
                top_issue = result['top_issues'][0]
                logger.info(f"      Top Issue: {top_issue['type']} ({top_issue['confidence']:.3f})")
    
    # Save results
    results_file = f"quick_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(results_file, 'w') as f:
        json.dump({
            'test_metadata': {
                'test_date': datetime.now().isoformat(),
                'improvements': 'Priority 1-3 fixes applied',
                'documents_tested': len(test_documents),
                'total_processing_time': total_time
            },
            'results': results
        }, f, indent=2)
    
    logger.info(f"\nüìÅ Results saved to: {results_file}")
    logger.info("üéØ" + "="*78 + "üéØ")

if __name__ == "__main__":
    # Remove verbose logging from the LLM analysis for this quick test
    logging.getLogger('classifier.hybrid_rag').setLevel(logging.WARNING)
    
    quick_test_improvements()