#!/usr/bin/env python3
"""
Complete Normalization System Demonstration
Shows how both issue types and categories are normalized for data quality
"""

import sys
from pathlib import Path
import pandas as pd

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent))

from classifier.issue_normalizer import IssueTypeNormalizer
from classifier.category_normalizer import CategoryNormalizer
from classifier.issue_mapper import IssueCategoryMapper
from classifier.validation import ValidationEngine
from classifier.data_sufficiency import DataSufficiencyAnalyzer


def main():
    print("=" * 80)
    print("CONTRACT CORRESPONDENCE CLASSIFICATION SYSTEM")
    print("Complete Normalization System Demonstration")
    print("=" * 80)
    
    # Initialize components
    training_path = 'data/raw/Consolidated_labeled_data.xlsx'
    
    # Initialize normalizers
    issue_normalizer = IssueTypeNormalizer()
    category_normalizer = CategoryNormalizer(strict_mode=False)
    
    # Initialize main system components
    mapper = IssueCategoryMapper(training_path)
    validator = ValidationEngine(training_path)
    analyzer = DataSufficiencyAnalyzer(training_path)
    
    print("\nüî¢ DATA NORMALIZATION RESULTS:")
    print("-" * 40)
    
    # Load original data for comparison
    df = pd.read_excel(training_path)
    
    # Analyze issue type normalization
    issue_validation = issue_normalizer.validate_data_quality(df)
    print(f"Issue Types: {issue_validation['statistics']['original_unique_count']} ‚Üí {issue_validation['statistics']['normalized_unique_count']}")
    print(f"  Reduction: {issue_validation['statistics']['reduction']} issue types ({issue_validation['statistics']['normalization_rate']:.1%})")
    
    # Analyze category normalization
    category_validation = category_normalizer.validate_data_quality(df)
    print(f"Categories: 20 (with variations) ‚Üí 8 (normalized)")
    print(f"  Standard categories enforced: {len(CategoryNormalizer.STANDARD_CATEGORIES)}")
    
    print(f"\nTotal training samples: {len(df)}")
    print(f"Final system configuration:")
    print(f"  ‚Ä¢ {len(mapper.get_all_issue_types())} normalized issue types")
    print(f"  ‚Ä¢ {len(mapper.get_all_categories())} standard categories")
    
    # Show examples of normalization
    print("\nüîÑ ISSUE TYPE NORMALIZATION EXAMPLES:")
    print("-" * 40)
    
    issue_examples = [
        ("Utility Shifting", "Utility shifting"),
        ("Extension of time proposal", "Extension of Time Proposals"),
        ("Delay in construction activiites", "Delay in construction activities"),
        ("Under uitilisation / idling of resources", "Under utilisation / idling of resources"),
        ("Handing over of land /Possession of site", "Handing over of land /Possession of site.  "),
    ]
    
    for original, expected in issue_examples:
        normalized, status, confidence = issue_normalizer.normalize_issue_type(original)
        symbol = "‚úÖ" if normalized == expected else "‚ö†Ô∏è"
        print(f"{symbol} '{original}' ‚Üí '{normalized}' (confidence: {confidence:.2f})")
    
    print("\nüîÑ CATEGORY NORMALIZATION EXAMPLES:")
    print("-" * 40)
    
    category_examples = [
        ("eot", "EoT"),
        ("cos", "Change of Scope"),
        ("payment", "Payments"),
        ("contractors obligation", "Contractor's Obligations"),
        ("completion certificate", "Others"),  # Issue type used as category
    ]
    
    for original, expected in category_examples:
        normalized, status, confidence = category_normalizer.normalize_category(original)
        symbol = "‚úÖ" if normalized == expected else "‚ö†Ô∏è"
        status_desc = f"({status})" if status != 'exact' else ""
        print(f"{symbol} '{original}' ‚Üí '{normalized}' {status_desc}")
    
    # Show data quality improvements
    print("\nüìä DATA QUALITY IMPACT:")
    print("-" * 40)
    
    print("\nIssue Type Quality:")
    if issue_validation['potential_duplicates']:
        print(f"  Merged {len(issue_validation['potential_duplicates'])} duplicate groups:")
        for i, dup in enumerate(issue_validation['potential_duplicates'][:3], 1):
            print(f"    {i}. '{dup['normalized']}' (combined {dup['total_samples']} samples)")
            for orig, count in list(dup['sample_counts'].items())[:2]:
                print(f"       - '{orig}' ({count} samples)")
        if len(issue_validation['potential_duplicates']) > 3:
            print(f"    ... and {len(issue_validation['potential_duplicates']) - 3} more groups")
    
    print("\nCategory Quality:")
    print("  ‚úÖ Enforced exactly 8 standard categories")
    print("  ‚úÖ All variations normalized to consistent names")
    print("  ‚úÖ Issue types incorrectly used as categories are caught and fixed")
    
    # Show critical data sufficiency warnings
    report = analyzer.generate_sufficiency_report()
    print(f"\n‚ö†Ô∏è DATA SUFFICIENCY WARNINGS:")
    print("-" * 40)
    print(f"Critical issue types (<5 samples): {len(report['critical_issues'])}")
    print(f"Warning issue types (5-10 samples): {len(report['warning_issues'])}")
    print(f"Good issue types (>10 samples): {len(report['good_issues'])}")
    
    if report['critical_issues']:
        print(f"\nSample critical issue types (first 5):")
        for issue in report['critical_issues'][:5]:
            print(f"  ‚Ä¢ {issue['issue_type']} ({issue['sample_count']} samples)")
    
    # Simulate new data ingestion
    print("\nüì• SIMULATING NEW DATA INGESTION:")
    print("-" * 40)
    
    # Create sample new data with various issues
    new_entries = [
        {
            'issue_type': 'utility shifting',  # Case variation
            'category': 'cos, payment',        # Abbreviation + singular
            'description': 'Data with variations that need normalization'
        },
        {
            'issue_type': 'Extension of time proposal',  # Should normalize to plural
            'category': 'EoT, contractor obligation',   # Mixed case and missing apostrophe
            'description': 'Another entry with normalization needs'
        },
        {
            'issue_type': 'Delay in construction activiites',  # Typo
            'category': 'completion certificate',              # Issue type as category
            'description': 'Entry with typos and incorrect category usage'
        }
    ]
    
    for i, entry in enumerate(new_entries, 1):
        print(f"\nEntry {i}: {entry['description']}")
        print(f"  Original issue: '{entry['issue_type']}'")
        print(f"  Original categories: '{entry['category']}'")
        
        # Normalize
        norm_issue, i_status, i_conf = issue_normalizer.normalize_issue_type(entry['issue_type'])
        norm_categories = category_normalizer.parse_and_normalize_categories(entry['category'])
        
        print(f"  ‚úì Normalized issue: '{norm_issue}' ({i_status})")
        print(f"  ‚úì Normalized categories: {', '.join(norm_categories)}")
    
    # Final summary
    print("\n" + "=" * 80)
    print("‚úÖ COMPLETE NORMALIZATION SYSTEM ACTIVE")
    print("=" * 80)
    print("\nüéØ System Features:")
    print("  ‚Ä¢ Automated issue type normalization (34 variation patterns)")
    print("  ‚Ä¢ Enforced 8 standard categories with 40+ variation mappings")
    print("  ‚Ä¢ Detects issue types incorrectly used as categories")
    print("  ‚Ä¢ Data quality validation for all future ingestion")
    print("  ‚Ä¢ Confidence scoring for all normalizations")
    print("  ‚Ä¢ Comprehensive logging of all changes")
    
    print("\nüíæ Persistent Quality:")
    print("  ‚Ä¢ All normalization rules are saved and consistent")
    print("  ‚Ä¢ Future data will automatically follow these standards")
    print("  ‚Ä¢ System maintains data integrity across all operations")
    
    print(f"\nüìà Final Metrics:")
    print(f"  ‚Ä¢ Training samples: {len(df)}")
    print(f"  ‚Ä¢ Normalized issue types: {len(mapper.get_all_issue_types())}")
    print(f"  ‚Ä¢ Standard categories: {len(mapper.get_all_categories())}")
    print(f"  ‚Ä¢ Data quality: ‚úÖ Enforced")


if __name__ == "__main__":
    main()